{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intoduction to NLP Fundamentals in Tensorflow\n",
    "\n",
    "Derive information from text or speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(y_true, preds):\n",
    "\n",
    "    return {\n",
    "        \"Accuracy Score\": (accuracy_score(y_true, preds) * 100),\n",
    "        \"Precision Score\": (precision_score(y_true, preds)* 100),\n",
    "        \"Recall Score\": (recall_score(y_true, preds)* 100),\n",
    "        \"F1-Score\": (f1_score(y_true, preds)* 100)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle's intro to NLP dataset - target [1 - disaster, 0 - not disaster]\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "test_data = pd.read_csv(\"data/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword location  \\\n",
       "2644  3796  destruction      NaN   \n",
       "2227  3185       deluge      NaN   \n",
       "5448  7769       police       UK   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle Data\n",
    "train_data_shuffled = train_data.sample(frac = 1, random_state=42)\n",
    "train_data_shuffled.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples from each class?\n",
    "train_data_shuffled.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "#DebateQuestionsWeWantToHear Why does #Saudi arabia and #Israel get away with mass murder?\n",
      "#Wahhabism #Zionism\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "#Sismo DETECTADO #JapÌ_n [Report 3] 01:02:17 Okinawa Island region M3.8 Depth 10km Maximum seismic intensity 3 JST #??\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "for some reason im listening to curfew overtime and stuck in a kodak over and over again\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "TV program I saw said US air plane flew to uranium mine in Fukushima and attacked by machine gun when student army were digging it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_data)-5)\n",
    "for row in train_data_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6851, Validation: 762\n"
     ]
    }
   ],
   "source": [
    "# Split training data into train and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_data_shuffled[\"text\"].to_numpy(),\n",
    "                                                                                train_data_shuffled[\"target\"].to_numpy(), \n",
    "                                                                                test_size=0.1, random_state=42)\n",
    "print(f\"Train: {len(train_sentences)}, Validation: {len(val_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the avg number of tokens (words) in each sequence of the training set\n",
    "\n",
    "max_lenght = round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))\n",
    "max_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 01:19:23.676599: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Setup the vectorization variables\n",
    "MAX_VOCAB_LENGTH = 10_000\n",
    "MAX_LENGTH = max_lenght\n",
    "OUTPUT_MODE = \"int\"\n",
    "\n",
    "\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=MAX_VOCAB_LENGTH, # how many words in the vocab\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    split=\"whitespace\",\n",
    "                                    ngrams=None,\n",
    "                                    output_mode=OUTPUT_MODE,\n",
    "                                    output_sequence_length=MAX_LENGTH, # how long should the sequences be\n",
    "                                    pad_to_max_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 74,   9,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "sample_sentence = \"There is a flood in my street\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layers\n",
    "\n",
    "Parameter to care for:\n",
    "* `input_dim`\n",
    "* `output_dim`\n",
    "* `input_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "OUTPUT_DIM = 128\n",
    "\n",
    "embedding = layers.Embedding(input_dim = MAX_VOCAB_LENGTH,\n",
    "                            output_dim = OUTPUT_DIM,\n",
    "                            input_length = MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Time collapse is such a cool video technique.  https://t.co/upLFSqMr0C    \n",
      "\n",
      "Embedded Sentence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.02616053, -0.03138056,  0.01228428, ..., -0.02613548,\n",
       "         -0.0083745 ,  0.00446911],\n",
       "        [ 0.032688  , -0.01101407,  0.01527759, ...,  0.00511795,\n",
       "         -0.01518694,  0.04057634],\n",
       "        [-0.03250631,  0.01465285, -0.01969502, ..., -0.02715397,\n",
       "          0.01426859, -0.00975158],\n",
       "        ...,\n",
       "        [-0.00662961,  0.03503707,  0.04418975, ..., -0.00381732,\n",
       "         -0.00194566,  0.00450187],\n",
       "        [-0.00662961,  0.03503707,  0.04418975, ..., -0.00381732,\n",
       "         -0.00194566,  0.00450187],\n",
       "        [-0.00662961,  0.03503707,  0.04418975, ..., -0.00381732,\n",
       "         -0.00194566,  0.00450187]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original Text:\\n {random_sentence}\\\n",
    "    \\n\\nEmbedded Sentence:\")\n",
    "\n",
    "# Embed\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Text Data - Running Multiple Experiments\n",
    "\n",
    "* Model 0: Naive Bayes (baseline)\n",
    "* Model 1: Feed Forward Neural Network (dense network)\n",
    "* Model 2: LSTM Model (RNN)\n",
    "* Model 3: GRU Model (RNN)\n",
    "* Model 4: Bidirectional-LSTM Model (RNN)\n",
    "* Model 5: 1-D Convolutional Network (CNN)\n",
    "* Model 6: Transfer Learning (Tensorflow Hub)\n",
    "* Model 7: Model 6, only 10% of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Naive Bayes - Baseline Model\n",
    "* `Multinomial Navie Bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create Tokenization and Modelling Pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert text to numbers\n",
    "    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model achieves an accuracy of: 79.27\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the baseline model\n",
    "\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Baseline model achieves an accuracy of: {baseline_score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy Score': 79.26509186351706,\n",
       " 'Precision Score': 88.6178861788618,\n",
       " 'Recall Score': 62.643678160919535,\n",
       " 'F1-Score': 73.4006734006734}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Baseline Results\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_results = evaluate(y_true=val_labels,\n",
    "                            preds=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Feed-Forward Neural Net (Dense Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorboard Callback \n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save TensoBoard API\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape = (1, ), dtype = tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name = \"model_1_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 15)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_1.compile(loss = \"binary_crossentropy\",\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20220527-011926\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 22ms/step - loss: 0.6128 - accuracy: 0.6910 - val_loss: 0.5375 - val_accuracy: 0.7533\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.4421 - accuracy: 0.8196 - val_loss: 0.4669 - val_accuracy: 0.7861\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.3481 - accuracy: 0.8605 - val_loss: 0.4573 - val_accuracy: 0.7874\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.2858 - accuracy: 0.8897 - val_loss: 0.4643 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.2389 - accuracy: 0.9121 - val_loss: 0.4860 - val_accuracy: 0.7913\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "model_1_history = model_1.fit(\n",
    "    x = train_sentences,\n",
    "    y = train_labels,\n",
    "    epochs  = 5,\n",
    "    validation_data = (val_sentences, val_labels),\n",
    "    callbacks = [create_tensorboard_callback(dir_name = SAVE_DIR,\n",
    "                                            experiment_name = \"model_1_dense\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_preds_probs = model_1.predict(val_sentences)\n",
    "model_1_preds_probs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 1., 1., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Porbs to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_preds_probs))\n",
    "model_1_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_0 Results:\n",
      " {'Accuracy Score': 79.26509186351706, 'Precision Score': 88.6178861788618, 'Recall Score': 62.643678160919535, 'F1-Score': 73.4006734006734}\n",
      "Model_1 Results:\n",
      " {'Accuracy Score': 79.13385826771653, 'Precision Score': 84.36363636363636, 'Recall Score': 66.66666666666666, 'F1-Score': 74.47833065810595}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model_1_results = evaluate(val_labels, model_1_preds)\n",
    "print(f\"Model_0 Results:\\n {baseline_results}\")\n",
    "print(f\"Model_1 Results:\\n {model_1_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e2db2fe913eb9e94297199806106f5b16655b08f59047963fa319e06216a9ba"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('3.10.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
